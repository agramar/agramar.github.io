---
layout: post
title: 03 Spark Shell
category: Spark
tags:
- Spark
- Shell
---
# 1. 스파크 셸이란?
    - 정의

# 2. 셸 실행
```
# cd ${SPARK_HOME}
# spark-shell
```

- Spark shell application UI
- 셸을 실행시킨 상태로 브라우져로 4040포트로 접속하면 Spark shell application UI에 접속 가능


# 3. 로그 설정 변경
- 보다 상세한 로그 확인을 위해 로그레벨을 WARN에서 INFO로 변경
```
# cd ${SPARK_HOME}/conf
# cp ./log4j.properties.template log4j.properties
# vim log4j.properties

"log4j.logger.org.apache.spark.repl.Main=WARN" 을
"log4j.logger.org.apache.spark.repl.Main=INFO" 로변경
```

# 4. 스파크 셸에서 스파크 컨텍스트 확인
```
# spark-shell
scala> sc
res0: org.apache.spark.SparkContext ...
```

# 5. 단어 수 세기 예제를 스파크 셸을 이용해 실행
```
# spark-shell

scala> val file = sc.textFile("file:///usr/local/src/spark-2.2.0-bin-hadoop2.7/README.md")
scala> val words = file.flatMap(_.split(" "))
scala> val result = words.countByValue
scala> result.get("For").get

res1: Long = 3
```

# 6. 스파크 셸 실행 옵션
1. 스파크 셸이 동작할 떄 필요한 클래스와 라이브러리에 관한 옵션
2. 애플리케이션 실행과 관련된 옵션
3. 클러스터 동작과 관련된 옵션
    - 실행옵션을 통해 로컬/클러스터 등으로 선택해 구동 가능

# References
- [빅데이터 분석을위한 스파크2 프로그래밍(백성민, 위키북스, 2017)](http://wikibook.co.kr/spark/)
